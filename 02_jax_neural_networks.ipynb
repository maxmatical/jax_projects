{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_jax_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN25WAnPMGjRrM3vm37v7F5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxmatical/jax_projects/blob/master/02_jax_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQOA1P2BgIbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubcZcvZDgX07",
        "colab_type": "text"
      },
      "source": [
        "defining hyperparams for NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8_T4Kr1gT5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_layer_params(m, n, key, scale = 1e-2):\n",
        "  # initialize the weights and bias for a hidden layer\n",
        "  w, b = random.split(key)\n",
        "  return scale*random.normal(w, (n,m)), scale*random.normal(b, (n,))\n",
        "\n",
        "def init_nn_params(sizes, key):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "# definig parameters of the nn\n",
        "\n",
        "szs = [28*28, 512, 256, 10] # flattened input will be 28*28\n",
        "param_scale = 0.1\n",
        "lr = 0.01\n",
        "n_epochs = 10\n",
        "bs = 128\n",
        "n_classes = 10\n",
        "\n",
        "params = init_nn_params(szs, random.PRNGKey(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE6vKeGRjTxT",
        "colab_type": "text"
      },
      "source": [
        "defining NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVjt2-FejTGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def net(params, input):\n",
        "  # forward pass for a sinlge input\n",
        "  out = input\n",
        "  for w, b in params[:-1]: # go through hidden layers until classifier layer\n",
        "    out = np.dot(w, out) + b\n",
        "    out = relu(out)\n",
        "  w_clas, b_clas = params[-1]\n",
        "  logits = np.dot(w_clas, out)+b_clas\n",
        "  # return logits\n",
        "  return logits -logsumexp(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQR6s7ugj_8d",
        "colab_type": "text"
      },
      "source": [
        "checking to see if it works on a single sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tuxs74fTjHiN",
        "colab_type": "code",
        "outputId": "70d01e90-4f08-4217-9a46-cc770ad677c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rand_input = random.normal(random.PRNGKey(1), (28*28,))\n",
        "preds = net(params, rand_input)\n",
        "print(preds.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6H9OpdPkVAw",
        "colab_type": "text"
      },
      "source": [
        "using `vmap` to generate batch predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVMl-pTCkR0E",
        "colab_type": "code",
        "outputId": "c25f53f2-8e1e-4cbf-d9d5-5d5583e873b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "forward_batch = vmap(net, in_axes = (None, 0))\n",
        "rand_input_batch = random.normal(random.PRNGKey(1), (128, 28*28,))\n",
        "batch_preds = forward_batch(params, rand_input_batch)\n",
        "print(batch_preds.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omWdK3FclJ-H",
        "colab_type": "text"
      },
      "source": [
        "defining metrics and loss function\n",
        "\n",
        "a cross_entropy loss would look something like\n",
        "\n",
        "``` \n",
        "def cross_entropy_loss(logits, labels):\n",
        "    log_softmax_logits = jax.nn.log_softmax(logits)\n",
        "    num_classes = log_softmax_logits.shape[-1]\n",
        "    one_hot_labels = common_utils.onehot(labels, num_classes)\n",
        "    return -jnp.sum(one_hot_labels * log_softmax_logits) / labels.size\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yix7q7-QkzrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(x, k, dtype = np.float32):\n",
        "  # create 1-hot encoding of size k\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "\n",
        "def accuracy(params, input, targets):\n",
        "  target_class = np.argmax(targets, axis=1)\n",
        "  pred_class = np.argmax(forward_batch(params, input), axis = 1)\n",
        "  return np.mean(pred_class==target_class)\n",
        "  \n",
        "# def loss(params, x, targets):\n",
        "#   preds = forward_batch(params, x)\n",
        "#   return -np.mean(preds*targets)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "when the output is only logits\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def loss(params, input, targets): # cross entropy loss for logits\n",
        "  logits = forward_batch(params, input)\n",
        "  preds = jax.nn.log_softmax(logits)\n",
        "  return -np.mean(targets*preds)\n",
        "\n",
        "@jit\n",
        "def update(params, x, y):\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  return [(w-lr*grad_w, b-lr*grad_b) \n",
        "          for (w, b), (grad_w, grad_b) in zip(params, grads)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da2eVGIComj",
        "colab_type": "text"
      },
      "source": [
        "Using TF Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fjeR0BKCnFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "mnist, info = tfds.load(name=\"mnist\", batch_size =-1, data_dir=data_dir, with_info = True)\n",
        "mnist = tfds.as_numpy(mnist)\n",
        "train, test = mnist['train'], mnist['test']\n",
        "n_classes = info.features['label'].num_classes\n",
        "h,w,c = info.features['image'].shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hxVCpmBDVzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshaping data into 1 contiguous vector\n",
        "# and one hot encoding labels\n",
        "\n",
        "train_img, train_label = train['image'], train['label']\n",
        "train_img = np.reshape(train_img, (len(train_img), h*w*c))\n",
        "train_label = one_hot(train_label, n_classes)\n",
        "\n",
        "test_img, test_label = test['image'], test['label']\n",
        "test_img = np.reshape(test_img, (len(test_img), h*w*c))\n",
        "test_label = one_hot(test_label, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC7GmaTAEYK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b4e6e071-c474-46bb-acb7-249e168c034b"
      },
      "source": [
        "# validate\n",
        "print('Train:', train_img.shape, train_label.shape)\n",
        "print('Test:', test_img.shape, test_label.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: (60000, 784) (60000, 10)\n",
            "Test: (10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJlGQ-jVF2pm",
        "colab_type": "text"
      },
      "source": [
        "training NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9DyrFLZEuHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "cf539e42-d3a8-449c-e653-a94500699daf"
      },
      "source": [
        "import time\n",
        "\n",
        "def get_train_batches():\n",
        "  ds = tfds.load(name=\"mnist\", split=\"train\", as_supervised=True, data_dir=data_dir) # as_supervised = True returns tuple instead of a dict\n",
        "  ds = ds.batch(bs).prefetch(1)\n",
        "  return tfds.as_numpy(ds)\n",
        "\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  start = time.time()\n",
        "  for x, y, in get_train_batches():\n",
        "    x = np.reshape(x, (len(x), h*w*c))\n",
        "    y = one_hot(y, n_classes)\n",
        "    params = update(params, x, y)\n",
        "  epoch_time = time.time()-start\n",
        "  train_loss = loss(params, train_img, train_label)\n",
        "  test_loss = loss(params, test_img, test_label)\n",
        "  train_acc = accuracy(params, train_img, train_label)\n",
        "  test_acc = accuracy(params, test_img, test_label)\n",
        "\n",
        "  # printing statistics\n",
        "  print(f\"epoch {epoch+1} completed in {epoch_time} seconds\")\n",
        "  print(f\"train loss: {train_loss}; training accuracy: {train_acc}\")\n",
        "  print(f\"val loss: {test_loss}; val accuracy: {test_acc}\")\n",
        "\n",
        "  # saving best params\n",
        "  if test_acc > best_acc:\n",
        "    best_params = params\n",
        "    print(f\"better model found at epoch {epoch+1}, saving model\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 completed in 10.527390718460083 seconds\n",
            "train loss: 0.029938776046037674; training accuracy: 0.9151999950408936\n",
            "val loss: 0.028568075969815254; val accuracy: 0.9201000332832336\n",
            "better model found at epoch 1, saving model\n",
            "epoch 2 completed in 8.731337308883667 seconds\n",
            "train loss: 0.023019123822450638; training accuracy: 0.9347833395004272\n",
            "val loss: 0.022458553314208984; val accuracy: 0.9355000257492065\n",
            "better model found at epoch 2, saving model\n",
            "epoch 3 completed in 9.47747278213501 seconds\n",
            "train loss: 0.019159628078341484; training accuracy: 0.9455500245094299\n",
            "val loss: 0.019136376678943634; val accuracy: 0.9463000297546387\n",
            "better model found at epoch 3, saving model\n",
            "epoch 4 completed in 9.177500486373901 seconds\n",
            "train loss: 0.016445571556687355; training accuracy: 0.9531166553497314\n",
            "val loss: 0.01683495007455349; val accuracy: 0.9517000317573547\n",
            "better model found at epoch 4, saving model\n",
            "epoch 5 completed in 9.285526990890503 seconds\n",
            "train loss: 0.01440518070012331; training accuracy: 0.959850013256073\n",
            "val loss: 0.0151604562997818; val accuracy: 0.956000030040741\n",
            "better model found at epoch 5, saving model\n",
            "epoch 6 completed in 9.03898310661316 seconds\n",
            "train loss: 0.012801471166312695; training accuracy: 0.9642500281333923\n",
            "val loss: 0.013880385085940361; val accuracy: 0.9598000645637512\n",
            "better model found at epoch 6, saving model\n",
            "epoch 7 completed in 8.590872049331665 seconds\n",
            "train loss: 0.011492796242237091; training accuracy: 0.9680333733558655\n",
            "val loss: 0.012882652692496777; val accuracy: 0.9628000259399414\n",
            "better model found at epoch 7, saving model\n",
            "epoch 8 completed in 8.46248459815979 seconds\n",
            "train loss: 0.010415885597467422; training accuracy: 0.9711000323295593\n",
            "val loss: 0.012091444805264473; val accuracy: 0.9643000364303589\n",
            "better model found at epoch 8, saving model\n",
            "epoch 9 completed in 8.57726240158081 seconds\n",
            "train loss: 0.009486308321356773; training accuracy: 0.9738333225250244\n",
            "val loss: 0.011434582062065601; val accuracy: 0.966200053691864\n",
            "better model found at epoch 9, saving model\n",
            "epoch 10 completed in 9.01668405532837 seconds\n",
            "train loss: 0.008695193566381931; training accuracy: 0.9764333367347717\n",
            "val loss: 0.010898163542151451; val accuracy: 0.9682000279426575\n",
            "better model found at epoch 10, saving model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fKWbqtMNN7Y",
        "colab_type": "text"
      },
      "source": [
        "Additional utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcFZOgcdHBRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3553d8a3-f401-4a32-dc81-9a972bdf7e3f"
      },
      "source": [
        "# saving params\n",
        "\n",
        "best_params # list of length 3"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC1uc37tNGZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}